\section{Introduction}
\label{sec:intro}

There is and will continue to be a constant competition between anti-virus tools and malwares.
Malwares grow exponentially~\cite{avtest} and place an imperative threat to human society. 
For example, more than 390000 new malwares are registered in AVTest institute every day~\cite{avtest}.
As another example, a new type of threat, ransomware, has caused more than 1 billion dollars this year~\cite{ransomware}. 
In order to keep up and detect these new types of malwares, anti-virus tools are also improving rapidly,
by constantly updating their signature database, by using more advanced techniques like deep learning~\cite{cylance}, or by utilizing more data. 

In order to improve anti-virus tools and defend against emerging threats from malwares, 
it is essential to understand malwares and existing anti-virus tools in the real world. 
Previous works on studying malwares and anti-virus engines do provide valuable 
insights~\cite{ZhouSP2012,GuptaComsnets2009, vendors-study} such as  
how malware writers create new malwares and how malwares escape from the detection of anti-virus engines.
However, these previous works only studied limited malwares
%and fail to understand malwares in a large scale, 
and did not provide insights into the relationship between malwares and anti-virus engines. 

We propose to use the vast amount of existing ``big data'' 
to study real-world malwares and their relationship with anti-virus engines.
To conduct such study, we utilized an open data repository
that contains billions of real-world malwares, {\em \vt}.

\vt{} is a free online malware scanning servive
that applies a set of state-of-the-art anti-virus engines to analyze user-submitted files 
and sends a detection report back to user.
\vt{} provides public access to all its submitted files and analysis results. 
\vt{} is a valuable resource to study and 
understand real-world malwares and anti-virus engines. 

First, there are huge amount of suspicious files submitted to VirusTotal. 
%As shown in Figure~\ref{fig:subnum}, 
For example, within our data collection window, 
there were around 40 million submissions to \vt\ each month. 
These submissions cover a large variety of file types, and 
are conducted by a large variety of \vt\ users from all over the world. 
This amount of diverse data on VirusTotal serves as a good representation of malwares in the real world.  

Second, for almost all submissions, 
VirusTotal applies no less than 50 state-of-the-art anti-virus engines to analyze them. 
VirusTotal keeps detailed detection results and provides an open access to these results. 
Analyzing historical detection results can help capture how anti-virus engines evolve over time. 

Third, VirusTotal provides rich metadata for each submission. 
Besides the detection results of various anti-virus engines, 
VirusTotal also provides file type information, submitter information, and a hash string of the original submitted file.
These types of information is valuable to study 
%which can help categorize malwares, 
%source ID (country), which can help understand popularity of malwares, 
%ssdeep digest string, by using which we can calculate code similarity without accessing binary executable, and so on. 

Unfortunately, there has only been limited attention to \vt\ in the past. 
%Researchers have used \vt\ as a testing platform when \yiying{developing malwares?}
Researchers tried to capture malware writers who leverage \vt{} as the testing platform during malware development~\cite{huangvt2016bigdata, neeles}. 
Anti-virus vendors use \vt\ to detect possible false positives and false negatives in their products. 
But none of them study the rich malware data provided by \vt\ in a large scale. 

We collected 4 months of metadata of all submissions to VirusTotal 
and conducted an extensive study on them.
%Following previous works~\cite{SongAPsys2016} on studying VirusTotal,
%We focus our effort on Windows \textit{Portable Executable} ({\em \pe}) files, 
%which account for more than half of \vt{}’s submissions.
After collecting and pre-processing data from \vt, 
we first perform a set of basic analysis to gain an overall knowledge of the \vt\ data repository,
including how big the files submitted to \vt\ are, who submit to \vt, and how many submissions are labeled as malware.

On top of these basic findings, we developed a set of more advanced studies in three directions. 

First, we study the correlation between submissions’ metadata and their {\em detection rates}, 
the percentage of engines labeling a submission as malware. 
We found high correlation between detection rate 
and three factors: submission file size, the history of submitting a file, and the reputation of the submitters.
These results shed light on what types of submission are more likely to be malware.
With this result, future researchers and anti-virus vendors can have more guided direction 
into what they should have further investigation.
%help security experts invest their limited manual efforts, 
%and help anti-virus vendors identify possible false positives and false negatives in their products.   

Second, we study the question of whether or not different anti-virus vendors can influence each other
and if detection rate is a perfect measurement of the likelihood of a file being a true malware.
Anecdotely, anti-virus vendors frequently leverage VirusTotal to identify false negatives in their products, 
which are malwares detected by others’ products but not detected by their own products. 
To verify this hypothesis, we use the detection history from VirusTotal 
and developed statistical models based on a known technique from from the web mining area to quantify the influence between vendors. 
With this method, we confirmed that there do exist high influence between vendors;
certain vendors are highly influenced by the detection results of other vendors and use this information to change their detection results.
This result alerts \vt\ users and researchers to be more careful about the detection results from vendors 
and use detection rate more precautiously.
%Anti-virus vendors can rely on this technique to identify false negatives in their products. 

Third, we explore the feasibility to build malware classifiers or detectors based on ssdeep similarity. 
For each submission, ssdeep hash string is also provided by VirusTotal. 
The similarity between two ssdeep hash strings 
is a good estimation of similarity between the two original binary files.
Hash strings can be automatically calculated by ssdeep programs. 
If we can build malware detectors by using ssdeep hash strings, 
we can avoid manually extract signatures from malware samples.   
We design several experiments to evaluate our ssdeep-based malware classifiers. 
Our evaluation results show that precision of malware classifiers are 
bounded by percentage of tailing examples and more training data tends to bring better results.

Our study advances the understanding of malwares and anti-virus engines in the real world, 
and provides implications for future works to 
apply machine learning to malware detection in a large scale. 

