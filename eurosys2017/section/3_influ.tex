\section{Influence Propagation}
\label{sec:influ}

As discussed in Section~\ref{sec:meth}, many files are submitted to VirusTotal more than once, 
and more than 99\% submissions are analyzed by at least 50 anti-virus engines. 
We observe that some engines fail to identify some malwares during early submissions, 
but they catch up when analyzing later submissions. 
Anti-virus vendors frequently use VirusTotal to identify false negatives in their products, 
which are malwares they fail to detect but detected by other vendors. 
We hypothesize that there are influence among different anti-virus vendors.
This section presents our verifification of this hypothesis by modeling the cross-vendor influence.
We also provide the prediction model of whether an engine will identify a file as malware in the future
after labeling the file as benign.

\subsection{Influence Model}
\label{sec:model}
Influence propagation on social graph is a well-studied topic in the web mining area. 
Borrowing this idea, we use graphs to represent the relationship between vendors 
and model influence among different vendors based on static models 
originally proposed to analyze Flickr data~\cite{Influence}.
We now explain our graph-based model for anti-virus vendor influence.
% first overview static models in our usage scenario,
%and then we discuss how we evaluate static models on data we collect. 

We first connect all vendors with a complete directed graph $G = (V, E)$, 
where the nodes $V$ are vendors. 
The graph is complete, 
since we assume there is possible influence from any vendor to all other vendors.
We  there are action logs generated, 
when VirusTotal applies a set of anti-virus engines to analyze a submission. 
For each engine, there is an action log $(u, a, t)$ or an action log $(u, \bar{a}, t)$, 
representing whether or not $u$ takes action to identify the submission as a malware at time $t$. 
Since the goal of our study is to detect if the change from labeling a file as benign to labeling it as malware by a vendor is influenced by other vendors, 
we do not consider the case of changing from labeling benign to malware and assume that after a vendor labels a file as malware, it will not change its decision.
A similar model can be applied to study the latter type.
%{\color{red} Flipping related setting in our model can handle cases where a vendor $u$ labels a file as malware, 
%but changes its mind to label the file as benign later.}
%\yiying{why can we ignore this case?}

We use $A_u$ to represent the set of submissions identified by $u$ as malwares in its history
and $\bar{A}_u$ represents the set of submissions labeled as benign by $u$.
%the number of actions taken by $u$, 
%or the number of malwares identified by $u$. 
\yiying{I think all these $A$ should be a collection/set, not the total number, since you use $\in$ later}
\yiying{Verify that the above sentence is correct.}
Further, we define the set of actions taken by both $u$ and $v$ as $A_{u\&v}$ 
and the set of actions taken by either $u$ or $v$ as $A_{u|v}$.
Thus, $|A_{u|v}| =   |A_u| + |A_v| - |A_{u\&v}|$.
Finally, $A_{u2v}$ represents the set of actions propagated from $u$ to $v$. 
With these terms, we define {\em action propagation} as follows: 

\begin{definition}{Action Propagation:}
We say that an action $a$ propagated from $u$ to $v$ iff: (i) $\exists$ $(v, \bar{a}, t_i)$ $\in$ $\bar{A}_v$ 
and $(v, a, t_k)$ $\in$ $A_v$, with $t_i < t_k$; (ii) $\exists$ $(u, a, t_j)$ $\in$ $A_u$, with $t_j < t_k$ and $u != v$. 
\end{definition}
\yiying{change the above != to the proper math format}

We associate each edge $(u, v) \in E$ in the graph $G$ 
with an influence probability $p_{u,v}$,
which represents the probability that after $u$ takes an action $v$ will follow $u$ to take the same action. 
%Since the graph is a complete graph, 
%all other nodes are all $v$'s neighbors. 
For an action $a$, we use $S_v(a)$ to represent the set of $v$'s neighbors taking action $a$ before $v$ in time. 
The probability that $v$ will follow its neighbors to take the same action can then be calculated as:

\begin{equation} \label{eq:setp}
%$$p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})$$
p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})
\end{equation}

During training stage, we learn $p_{u,v}$ for each edge. 
During prediction stage, we test a node $v$ that has not taken the action $a$ yet
and predict if it will take this action following other nodes (anti-virus vendors). 
Specifically, we use a tunable threshold $\theta$
and predict $v$ will follow its neighbors to take the action $a$ in the future
if $P_v(S_v(a))>\theta$.

We designed four static models for the training stage, which differ in how to estimate $p_{u,v}$ by using the data in training set. 


{\bf Bernoulli distribution} estimates $p_{u,v}$ as the ratio of the number of actions 
propagated from $u$ to $v$ over the total number of actions taken by $u$.

$$p_{u,v} = \frac{|A_{u2v}|}{|A_u|}$$ 

{\bf Jaccard index} estimates 
$p_{u,v}$ as the number of actions propagated from $u$ to $v$ divided by 
the number of actions taken either by $u$ or by $v$.

$$p_{u,v} = \frac{|A_{u2v}|}{|A_{u|v}|}$$ 

When $v$ takes an action $a$, it may be influenced by the combination of all its neighbors $S_v(a)$ 
taking the action $a$ before $v$. Partial credit takes this intuition. 
Partial credit for $u$ who takes an action $a$ before $v$ is calculated as:

$$credit_{u,v}(a) = \frac{1}{|S_v(a)|}$$

{\bf Bernoulli distribution with partial credit} 
estimates $p_{u,v}$ as the sum of all partial credits taking by $u$ for actions propagated from $u$ to $v$, 
dividing by the number of actions taken by $u$. 

$$p_{u,v} = \frac{\sum\limits_{a \in A_{u2v}}{credit_{u,v}(a)}}{|A_u|}$$

{\bf Jaccard index with partial credit} 
estimates $p_{u,v}$ as the sum of all partial credits taking by $u$ for actions propagated from $u$ to $v$, 
dividing by the number of actions taken either by $u$ or by $v$. 

$$p_{u,v} = \frac{\sum\limits_{a \in A_{u2v}}{credit_{u,v}(a)}}{|A_{u|v}|}$$




\subsection{Model Evaluation}
\label{sec:predict}

We split all PE submissions we collect into training set and testing set, based on submitted files' sha256. 
If a sha256 starts with a numeric character, 
from ‘0’ to ‘9’, we put all its related submissions to training set, 
otherwise, we put all its related submissions to testing set.  
For a submitted file, all its submissions are put together either in training set or in testing set. 

The training process is implemented by several stages of map, filter, and reduce. 
We first reduce submission records according to submitted files. 
We sort submission records for each file chronologically, 
and filter out files without action propagation and all their submissions. 
For each file, we compute four maps based on its submissions.
There is one one-dimension map, 
containing the number of actions taken by each vendor, 
or the number of malwares identified by each vendor. 
There are three two-dimension maps, 
containing an entry for each pair of vendors $(u,v)$, 
representing actions taken by both $u$ and $v$, actions propagated from $u$ to $v$, 
and partial credits taken by $u$ for actions propagated from $u$ to $v$. 
Finally, we reduce these 4 maps from different files, 
and calculate $p_{u,v}$ for the four static models. 

After training the four static models, 
we filter out engines taking less than 10000 actions,
since for engines taking too few actions, results may not be reliable. 
There are 59 engines left.
For confidential reasons, we omit vendors’ name in the following discussion, 
and use number from 0 to 58 to represent each engine. 

For each static model, 
we put all learned $p_{u,v}$ in a influence table, 
by using $u$ as row number and $v$ as column number.
For any $p_{u,v}$, bernoulli distribution has the largest value, 
jaccard index has the second largest value, 
bernoulli with partial credit has the third largest value,
and jaccard index with partial credit has the least value. 

\input{section/fig_heatmap}

We draw heat maps for all learned influence tables in Figure~\ref{fig:heat}. 
Given a $cell(u, v)$ with $u$ as row number and $v$ as column number, 
red color means more influence from $u$ to $v$. 
The four heat maps share a similar pattern.
There are columns with almost all cells in red color, 
which means there are vendors influence by all other vendors. 
There are no rows with all cells in red color, 
and this means that there are no vendors which can influence all other vendors. 

For each row in all influence tables, we sum all its number together. 
The results can serve as a quantitative measurement for engines' influence in malware detection community. 
Influence rankings from all the four static models also share a similar pattern. 

The testing process is also implemented by several stages of map, filter, and reduce.
Same as what we did during training, 
we first reduce all submissions based on submitted files, 
filter out files without action propagation and all their related submissions, 
and sort submissions chronologically for each file. 
Before processing a submission, we use Equation~\ref{eq:setp} to
calculate $p_v(S_v(a))$ for any engine $v$, 
which labeled the submitted file as benign when analyzing early submissions, 
but have not labeled the file as malware. 
$S$ is the set of engines which labeled the file as malwares before.
We compare $p_v(S_v(a))$ with a given threshold $\theta$ to decide whether we will predict 
$v$ will label the file as malware, and we check what $v$ 
really does in the submission to count true positive (TP) and false negative (FN). 
After processing all submissions for a file, 
we calculate $p_v(S_v(a))$ for any engine $v$, 
which labels the file as benign, but have not labeled the file as malware.
We compare $p_v(S_v(a))$ with $\theta$ to count false positive (FP) and true negative (TN).
In the final stage, we reduce TP, TN, FP, and FN numbers from different files.


\input{section/fig_predict}

We change the probability threshold $\theta$ from 0.1\% to 99.9\%, 
by using 0.1\% as step. 
We depict the performance of the four static models by using ROC curves, 
with true positive rate ($TPR = TP/(TP+FN)$) as x-axis, 
and false positive rate ($FPR = FP/(FP + FN)$) as y-axis. 
ROC curves for the four static models are shown in Figure~\ref{fig:predict}. 
The closer the hump of the ROC curve to $(0,1)$, the better performance.
Bernoulli with partial credit is as good as jaccard index, 
and both of them are better than bernoulli and jaccard index. 


\subsection{Discussion}

There are also time models proposed by~\citet{Influence}.
When analyzing Flickr data, time models leverage accurate action time to provide better prediction performance. 
However, we do not use time models, 
because time information we access is when a submission is conducted, 
or when an engine analyzes a submission, 
is not when an engine changes its detection label for a file.
The time information we have can only provide a relative order about when each vendor identifies a file as a malware.  

\underline{Limitation.}
Our data collection ends on 09/06/2016. 
We could count extra FPs, where we predict engines will label files as malwares, engines have not, but will do in the future, 
and TNs, where we predict engine will not label, engines have not, but will in the future. 
Monitoring \vt for a longer time will improve the precision of our prediction models. 

\underline{How to use?}
Our trained models can serve as 
a quantitative measurement for vendors’ influence in malware detection community. 
Previously, when combining results from different vendors, 
security experts simply treat each vendor equally and use the percentage of 
vendors labeling a file as malware as likelihood of the file to be a malware. 
Our model can provide a weigh, when considering results from different vendors.  
For anti-virus vendors, the prediction part of our models can help alarm possible false negatives in their products.
