\section{Correlation between Submission Properties and Detection Rate}
\label{sec:corr}
This section presents our study of the correlation between various properties of
\pe\ submissions and their detection rate. 
Detection rate is what most \vt\ users refer to decide whether their submissions
are benign or malware and the first thing that a user of \vt\ uses. 
Therefore, it is important to study what factors affect or correlate to detection rates. 
We focus on \vt\ submissions' properties, which can be obtained without executable binary files and more lightweight than static features.
The results provide a lightweight mechanism to guide security researchers 
and vendors to have targeted investigation over
{\em suspicious} files, 
files that have the factors that we identify as highly correlated to detection rate.

We studied a range of properties and their correlation with detection rate
and found that three factors have higher correlation:
submission file size,
historical submission properties, and the reputation of source IDs.
We also built a regression model based on these three factors to predict detection rate.
We present these correlation study results and our regression model in this section.
In the next section, we will present our further analysis of what can affect the detection result of anti-virus engines
and if detection rate is a perfect measurement of the likelihood of malware.


\input{section/fig_size}

\subsection{File Size}
\label{sec:size}
Anecdotally, \pe\ malwares are most likely to have small to medium size. 
To verify this, we analyzed the relationship between submission file size and detection rate. 
Figure~\ref{fig:size} presents the average detection rate and 
the 95\% confidence interval for different file sizes.
A wider confidence interval implies that
the calculated average detection rate is farther away from the real average detection rate.
We use discrete file sizes of powers of two in our analysis and in this graph,
i.e., we calculate the log2 of each original file size and round it to the nearest 0.5 value.
Overall, we find that files with size from 90KB to 4MB have higher detection rate, more than 20\% on average. 
Except for the last two points which represent a small amount of files that are bigger than 1GB, 
all other sizes have high confidence.   

{\bf Observation 4:} 
{\em \pe\ malwares are mostly likely to have small to medium size.}

An immediate question to raise is whether the high detection rate of files with small to medium size
is because these files also contribute to most of the submissions as shown in Figure~\ref{fig:pesize}.
As we will discuss in Section~\ref{sec:history}, the correlation of submissions and detection rate is more complex and non-linear.
Thus, there is a more fundamental reason behind the file size correlation with detection rate.
One likely reason of this correlation is that files that are too small are not enough to express the
malware functions while files that are too big are difficult to spread.

\subsection{Submission History}
\label{sec:history}


\input{section/fig_SubHistory}

As discussed in Section~\ref{sec:basicanal}, there are files that have been submitted more than once to VirusTotal. 
It is worthwhile to investigate this {\em history of submission} and how history affects future.
We study the correlation between submission history and the detection rate of the current submission.
Among the different types of historical information that we study,
we find that the number of submissions made in history has higher correlation to detection rate.

%\noindent{\textit{\underline{The number of submissions in history.}}}
To study the submission history, we first sort all submissions for each file chronologically
and then collect the number of all submissions made before submission $s$ for each submission $s$ in \vt.
Next, we calculate the correlation between detection rate and the number of submission in history.
Figure~\ref{fig:hisnum} plots how detection rates change over the number of historical submissions.
Interestingly, there are two main ranges that steadily increase and there are three drops in the beginning, middle, and end.

To explain this effect, we first look at the two factors that can both influence detection rates:
the percentage of benign files submitted and the amount of anti-virus engines labeling the submission as malware.
%Two factors can influence detection rates and explain the above effect.
Obviously, with more benign files submitted, detection rate will decrease
and with more engines labeling malwares, detection rate will increase. 
In the the two stages of detection rate increasing, 
with more submissions, more engines are able to identify malwares 
and the increase of percentage of engines identifying submitted malwares dominates. 
In the range that detection rate drops, 
VirusTotal users stop submitting files that have already been identified as malwares,
and the increase of percentage of benign files submitted to VirusTotal dominates. 



{\bf Observation 5:} 
{\em The number of submissions made in history has correlation with detection rate, but the correlation is non-linear and is affected by multiple factors.}


\subsection{Reputation of Source ID}
\label{sec:reputation}

\input{section/fig_IDReputation}
Different users of online services such as \vt\ often have different {\em reputation} 
because of different motivations, objectives, and backgrounds.
For example, some users can randomly trying out \vt\ with no specific objective and thus submit random files,
while other users have clear goals and motivations and thus only submit suspicious files.
The rationale behind reputation is that \vt{} users would have a roughly constant submission pattern, 
and it is promising to use their historical submission to predict their future submission.
Previous work~\cite{GuoICSE2010} reported that there is a correlation between bug reporter's reputation and the likelihood for the bug being fixed. 
We also observe correlation between the reputation of source ID and submission's detection rate. 

Intuitively, a user that have submitted more files that were detected as malware suggest 
that she is more likely to use \vt\ to detect malware in suspicious files 
and thus should be given higher reputation.
To quantify this, we define the reputation of a source ID as the follow.
The reputation of a source ID is not a fixed value and can change over time. 

%\theoremstyle{definition}
\begin{definition}{Reputation:}
Given a submission $S$ with source ID $N$, 
we define the reputation of $N$ when conducting the submission $S$ as the average detection rate for all submissions conducted by $N$ before $S$. 
If $N$ did not make any submission before, we define the reputation to be $-1$. 
\end{definition}

Before conducting our source ID reputation analysis, we first filter out submissions
without source ID information, and submissions by source ID with more than 1 million submissions (bogus or robots).
To calculate source ID reputation, we first sort all submissions from the same source ID chronologically, 
and calculate reputation for each source id when conducting each submission. 
We further separate normal users from vendors and analyze the correlation of their ID reputation and detection rate.

Figure~\ref{fig:idreputation} plots the average detection rate and the 95\% confidence interval 
as the source ID reputation increases for normal users and for anti-virus vendors.
We round up reputation values to their nearest 0.05 values. 

Detection rate steadily increase as the reputation increases from 0 to 1 for vendors and from 0 to 0.8 for normal users.
Except the points when reputation is 1, all other results have high confidence.
Interestingly, for both normal user and vendors the detection rate with reputation -1 is higher than with reputation 0,
which means that the first submissions conducted by all source IDs (reputation -1) 
are overall have higher detection rate than 
the submissions conducted by IDs that have only submitted benign files (reputation 0).

For normal users, the detection rate drops when reputation is greater than 0.8.
This means that for normal users, it is difficult for them to always submit suspicious files detected by almost all vendors.
Vendors do not have this behavior and their detection rate always increases with higher reputation (other than reputation -1).


{\bf Observation 6:} 
{\em There is a high correlation between user reputation and detection rate of their submissions for both normal users and anti-virus vendors.}


\subsection{Regression Model}

We build a linear regression model to predict the detection rate for a submission, 
by using factors we discussed above.  
We filter submissions without source ID reputation information, as we discussed in Section~\ref{sec:reputation}. 
We then randomly divide remaining submissions into training set and testing set with equal size. 
We choose random guess as baseline model, 
which is to guess the detection rate for any submission 
as the mean of detection rate for all training submissions. 
We calculate the mean absolute error (mae)~\cite{mae} 
and the mean squared error (mse)~\cite{mse} to compare our regression model and baseline model. 

For in sample testing, mae and mse for our regression model are 0.2227 and 0.0718 respectively, 
while for baseline model they are 0.3345 and 0.1327. 
For out of sample testing, mae and mse for our regression model is 0.2223 and 0.0716 respectively, 
and they are 0.3342 and 0.1325 for baseline model. 
For both in sample testing and out of sample testing, our regression model can beat baseline model significantly. 


{\bf Observation 7:} 
{\em Regression model based on studied factors can help the prediction of detection rate for malware submissions.}


\subsection{Discussion}


From our analysis, file size, number of submissions in history, 
and source ID reputation are all highly correlated with detection rate.
Our regression model based on these three factors can effectively help predict detection rate for \vt\ submissions, and this also verifies our correlation studying results. 
All the three studied factors can be obtained without analyzing binary executable files.
These factors plus our regression model provide a lightweight estimation or prediction for future submissions.  
Doing so can help reduce researchers' and vendors' effort to focus on more interesting files---files that are more likely to be malicious.
Anti-virus vendors can also inspect results which are variant from our prediction results 
to identify possible false positives or false negatives in their products. 
File size information can be accessed in \vt~'metadata. 
Number of submissions in history and source ID reputation are computed by ourselves. 
Our studying results suggest that \vt should also track number of submissions in history for each file and reputation for each source ID in the future. 

